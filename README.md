# PREDINHIB_MTB

**PREDINHIB_MTB** is a machine learning pipeline for predicting compound inhibition and MIC values against *Mycobacterium tuberculosis* (M. tuberculosis), leveraging curated assay descriptions processed through Large Language Models (LLMs).

## ğŸ” Overview

This repository supports two key tasks:
1. **MIC Regression** using curated, biologically meaningful datasets.
2. **Multi-task Classification** of compound activity across different resistance profiles (Non-Resistant, Resistant, MDR).

We use LLMs to extract structured metadata from free-text assay descriptions (e.g., from ChEMBL), enabling biologically consistent dataset construction and improved model performance.

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chemblapi/           # ChEMBL data for Mycobacterium tuberculosis
â”‚   â”œâ”€â”€ cv/                  # Repeated 5x5 cross-validation folds
â”‚   â”œâ”€â”€ llama/               # Llama3.3 input/output
â”‚   â””â”€â”€ res_mdr_xdr/         # Data for multi-task classification
â”‚
â”œâ”€â”€ figures/                 # Generated plots and figures
â”‚
â”œâ”€â”€ models/                  # Trained models and configuration files
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ 0_chembl_download.ipynb         # Download data from ChEMBL
â”‚   â”œâ”€â”€ 1_data_exploration.ipynb        # Initial dataset inspection
â”‚   â”œâ”€â”€ 2_data_cleaning.ipynb           # Filtering and cleaning
â”‚   â”œâ”€â”€ 3_llama_processing/
â”‚   â”‚   â””â”€â”€ 3_llama_processing.py       # Llama3.3 metadata extraction
â”‚   â”œâ”€â”€ 4_data_splitting/
â”‚   â”‚   â”œâ”€â”€ 4_1_llama_results_exploration.ipynb  # Metadata exploration
â”‚   â”‚   â””â”€â”€ 4_2_ml_preparation.ipynb             # ML dataset preparation
â”‚   â”œâ”€â”€ 5_h37rv_nr_raw/
â”‚   â”‚   â”œâ”€â”€ 5_0_folds_creation.ipynb             # Stratified fold generation
â”‚   â”‚   â”œâ”€â”€ fingerprints.ipynb                   # Morgan fingerprint generation
â”‚   â”‚   â”œâ”€â”€ kpgt_embeddings.ipynb                # KPGT embedding generation
â”‚   â”‚   â”œâ”€â”€ 5_2_kpgt_embeddings.ipynb             # Alternative KPGT generation
â”‚   â”‚   â”œâ”€â”€ fp_RF/run_all_rf.sh                   # Random Forest training
â”‚   â”‚   â”œâ”€â”€ fp_SVR/run_all_svr.sh                  # SVR training
â”‚   â”‚   â””â”€â”€ fp_DNN/run_all_dnn.sh                  # DNN training
â”‚   â””â”€â”€ 6_multitask/
â”‚       â”œâ”€â”€ data_curation.ipynb                  # Multi-task dataset curation
â”‚       â”œâ”€â”€ model_run.ipynb                      # Multi-task training/evaluation
â”‚       â””â”€â”€ multitask_test_set.csv               # Test set for classification
â”‚
â”œâ”€â”€ results/                    # Model predictions and CV results
â”œâ”€â”€ utils.py                     # Utility functions
â””â”€â”€ predinhib_env.yml            # Conda environment file
```

## ğŸ§ª How to Reproduce

### 1. Environment Setup

```bash
conda env create -f predinhib_env.yml
conda activate predinhib
```

### 2. Data Preparation

(Optional) Download and process the latest ChEMBL data:
1. `0_chembl_download.ipynb`
2. `1_data_exploration.ipynb`
3. `2_data_cleaning.ipynb`

*Note: This step is not needed for pure reproducibility of current results.*

### 3. Assay Metadata Extraction (LLaMA)

Run `3_llama_processing/3_llama_processing.py` to extract structured metadata (strain, resistance, mutant type) from assay descriptions.

We recommend running Ollama inside a container for GPU acceleration:

```bash
podman pull ollama
podman run -d --name ollama_cuda --privileged --gpus all -v ollama_data:/root/.ollama -p 11434:11434 docker.io/ollama/ollama
podman exec -it ollama bash
ollama pull llama3.3
exit
```

### 4. Metadata Exploration and Dataset Preparation

- Explore extracted metadata: `4_data_splitting/4_1_llama_results_exploration.ipynb`
- Prepare datasets for ML: `4_data_splitting/4_2_ml_preparation.ipynb`

### 5. Single-task Model Training and Evaluation

- Generate folds: `5_h37rv_nr_raw/5_0_folds_creation.ipynb`
- Generate fingerprints: `5_h37rv_nr_raw/fingerprints.ipynb`
- Generate KPGT embeddings: `5_h37rv_nr_raw/kpgt_embeddings.ipynb` or `5_2_kpgt_embeddings.ipynb`

**Note on KPGT embeddings**:  
Clone [KPGT](https://github.com/lihan97/KPGT.git), install its environment, and download its pre-trained model from [figshare](https://figshare.com/s/d488f30c23946cf6898f?file=35369662).  
Update the paths to the KPGT repository, model, and environment as needed.

- Train and evaluate models using the bash scripts:
  - `run_all_rf.sh` (Random Forest)
  - `run_all_svr.sh` (SVR)
  - `run_all_dnn.sh` (DNN)

### 6. Multi-task Model Training and Evaluation

Train and evaluate multi-task models using:
- `6_multitask/model_run.ipynb`

This focuses on classifying compounds into Non-Resistant, Resistant, and MDR categories.

## ğŸ“Š Models and Benchmarks

We benchmark:
- **Random Forest** (Morgan fingerprints)
- **SVR** (KPGT embeddings)
- **DNN** (KPGT embeddings, best performance)

Task performance:
- MIC regression: **RÂ² up to 0.65**, **MAE â‰ˆ 0.41**
- Resistance classification (multi-task): Non-Resistant / Resistant / MDR **F1 scores of 0.84, 0.81, and 0.67, respectively**.
    Classification threshold: **10 ÂµM**

## ğŸ“¬ Contact

For questions or feedback, contact **Nuno Alves** via GitHub or email: `id10075@alunos.uminho.pt`.