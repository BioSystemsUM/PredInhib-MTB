{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fda18f63cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonschema\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Set seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load descriptions\n",
    "descriptions_df = pd.read_csv(\"../../data/llama/unique_assay_descriptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to run ollama locally using podman/docker and cuda GPUs\n",
    "\n",
    "# podman pull ollama\n",
    "# podman run -d --name ollama_cuda --privileged --gpus all -v ollama_data:/root/.ollama -p 11434:11434 docker.io/ollama/ollama\n",
    "# podman exec -it ollama bash\n",
    "# ollama pull llama3.3\n",
    "# exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're testing our conversation! That's perfectly fine. How can I assist you today? Is there something specific you'd like to chat about or ask?\n"
     ]
    }
   ],
   "source": [
    "model = OllamaLLM(model=\"llama3.3\", base_url=\"http://localhost:11434\")\n",
    "response = model.invoke(\"testing\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answers(BaseModel):\n",
    "    mtb_strain: Union[str, bool] = Field(\n",
    "        description=\"strain of Mycobacterium tuberculosis\")\n",
    "    resistant_to: Union[str, bool] = Field(\n",
    "        description=\"drugs to which there is resistance.\")\n",
    "    mutant: Union[str, bool] = Field(description=\"if the strain is a mutant.\")\n",
    "    mutant_type: Union[str, bool] = Field(description=\"type of mutation.\")\n",
    "    checkerboard: Union[str, bool] = Field(description=\"checkerboard assay.\")\n",
    "    checkerboard_drug: Union[str, bool] = Field(\n",
    "        description=\"drug in checkerboard assay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/llama/prompt_template.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    response_template = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=response_template,\n",
    "    input_variables=[\"assay_description\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain chain\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_retries(description, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            output_dict = chain.invoke(description)\n",
    "            return output_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing description on attempt {attempt + 1}: {e}\")\n",
    "            time.sleep(delay)  # Wait before retrying\n",
    "    # Flag as error if all retries fail\n",
    "    #print(f\"Failed to process description {description} after {retries} attempts\")\n",
    "    return {'mtb_strain': 'error', 'resistant_to': 'error', 'mutant': 'error', 'mutant_type': 'error', 'checkerboard': 'error', 'checkerboard_drug': 'error'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10/10"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "test_df = descriptions_df.dropna(subset=['assay_description']).sample(n=10, random_state=42)\n",
    "\n",
    "# Dictionary to store results\n",
    "processed_descriptions = {}\n",
    "\n",
    "# Run through each of the 10 descriptions\n",
    "for index, description in enumerate(test_df['assay_description']):\n",
    "    print(f'\\rProcessing {index + 1}/10', end='')\n",
    "    processed_descriptions[description] = process_with_retries(description)\n",
    "\n",
    "def get_output_dict(description):\n",
    "    if pd.isna(description):\n",
    "        return {'mtb_strain': np.nan, 'resistant_to': np.nan, 'susceptible_to': np.nan, 'mutant': np.nan, 'mutant_type': np.nan, 'checkerboard': np.nan, 'checkerboard_drug': np.nan}\n",
    "    else:\n",
    "        return processed_descriptions[description]\n",
    "    \n",
    "    \n",
    "# Apply structured outputs back to the test dataframe\n",
    "output_df = test_df['assay_description'].apply(get_output_dict).apply(pd.Series)\n",
    "test_df[['mtb_strain', 'resistant_to', 'mutant', 'mutant_type', 'checkerboard', 'checkerboard_drug']] = output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_descriptions = {}\n",
    "\n",
    "for index, description in enumerate(descriptions_df['assay_description']):\n",
    "    print(f'\\rProcessing {index + 1}/{len(descriptions_df)}', end='')\n",
    "    processed_descriptions[description] = process_with_retries(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_dict(description):\n",
    "    if pd.isna(description):\n",
    "        return {'mtb_strain': np.nan, 'resistant_to': np.nan, 'susceptible_to': np.nan, 'mutant': np.nan, 'mutant_type': np.nan, 'checkerboard': np.nan, 'checkerboard_drug': np.nan}\n",
    "    else:\n",
    "        return processed_descriptions[description]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = descriptions_df['assay_description'].apply(get_output_dict).apply(pd.Series)\n",
    "descriptions_df[['mtb_strain', 'resistant_to', 'mutant', 'mutant_type', 'checkerboard', 'checkerboard_drug']] = output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predinhib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
